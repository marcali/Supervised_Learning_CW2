{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART III\n",
    "## Kernel perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(123)  # give a fixed seed for reproducability\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "zipcombo = np.loadtxt('zipcombo.dat')\n",
    "zipcombo_qater = zipcombo[:len(zipcombo)//4]\n",
    "training_data = np.loadtxt('dtrain123.dat')\n",
    "test_data = np.loadtxt('dtest123.dat')\n",
    "\n",
    "#some training data is continuous, need to make sure everything is either -1 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot first char\n",
    "def plot_char(char):\n",
    "    data = char[1:].reshape(16, 16)\n",
    "    plt.imshow(data, cmap='gray')\n",
    "    plt.title(f\"The number {char[0]}\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_char(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##helper functions(can be extracted into a separate class later)\n",
    "\n",
    "#polynomial kernel\n",
    "\n",
    "def polynomial_kernel(p, q, d):\n",
    "    #from the previous coursework:adding a constant to the dot product of the two vectors\n",
    "    return (1 + (p@q.T)) ** d\n",
    "\n",
    "def polynomial_kernel_3D(p, q, d):\n",
    "    #returns a 3D array of the polynomial kernel of the data, last dimension is the degree\n",
    "    kernels = np.empty((len(d), len(q), len(p)))\n",
    "    for i, degree in enumerate(d):\n",
    "        kernels[i, :, :] = (1 + np.dot(p, q.T)) ** degree\n",
    "\n",
    "    return kernels\n",
    "\n",
    "#Gaussian kernel\n",
    "def gaussian_kernel(p, q, sigma):\n",
    "    #from previous coursework: exp(-||p-q||^2 / 2*sigma^2)\n",
    "    return np.exp(-np.linalg.norm(p - q) ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "#convering the data to -1 and 1\n",
    "def mysign(data):\n",
    "    return np.where(data <= 0.0, -1.0, 1.0)\n",
    "\n",
    "#splitting data and labels\n",
    "def split_into_data_and_labels(data):\n",
    "    y = data[:,0].astype(int)\n",
    "    #convert to -1 and 1 here instead of in the loop\n",
    "    x = data[:, 1:]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1\n",
    "def train_perceptron(x, y, nb_epoches_max):\n",
    "    w = np.zeros(x.shape[1])\n",
    "    for e in range(nb_epoches_max):\n",
    "        mistakes = 0\n",
    "        nb_changes = 0\n",
    "        for i in range(len(x)):\n",
    "            if y[i] * np.dot(x[i], w) <= 0:\n",
    "                w += y[i] * x[i]\n",
    "                mistakes += 1\n",
    "                nb_changes += 1\n",
    "        if nb_changes == 0: break;\n",
    "    return w    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not online\n",
    "def train_kernel_perceptron(X, y, num_epochs=10):\n",
    "    num_samples = X.shape[0]\n",
    "    alpha = np.zeros(num_samples)\n",
    "\n",
    "    # Compute the kernel matrix\n",
    "    K = polynomial_kernel(X, X)\n",
    "\n",
    "    # Initialize mistake count and error rate lists\n",
    "    mistakes = []\n",
    "    error_rates = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        mistake_count = 0\n",
    "        for i in range(num_samples):\n",
    "            scores = sum(alpha[j] * y[j] * K[i, j] for j in range(num_samples))\n",
    "            predicted_class = np.sign(scores)\n",
    "            true_class = y[i]\n",
    "\n",
    "            if predicted_class != true_class:\n",
    "                alpha[i] += 1\n",
    "                mistake_count += 1\n",
    "\n",
    "        # Append mistake count and error rate for this epoch\n",
    "        mistakes.append(mistake_count)\n",
    "        error_rates.append(mistake_count / num_samples)\n",
    "\n",
    "    return alpha, K, mistakes, error_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using summation??????\n",
    "def train_kernel_perceptron_online_sum(X, y, alpha, K, i):\n",
    "    num_samples = X.shape[0]\n",
    "    scores = sum(alpha[j] * y[j] * K[i, j] for j in range(num_samples))\n",
    "    predicted_class = np.sign(scores)\n",
    "    true_class = y[i]\n",
    "\n",
    "    mistake = 0\n",
    "    if predicted_class != true_class:\n",
    "        alpha[i] += 1\n",
    "        mistake = 1\n",
    "\n",
    "    return alpha, mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kernel_perceptron_online(X, y, alpha, K, i, n_classes):\n",
    "    num_samples = X.shape[0]\n",
    "    scores = np.zeros(n_classes)\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        y_c = np.where(y == c, 1, -1)  # Binary labels for class c vs. all other classes\n",
    "        scores[c] = np.sum(alpha[c, :] * y_c * K[i, :])\n",
    "\n",
    "    c_max = np.argmax(scores)\n",
    "    predicted_class = c_max\n",
    "    true_class = y[i]\n",
    "\n",
    "    mistake = 0\n",
    "    if predicted_class != true_class:\n",
    "        alpha[true_class, i] += 1\n",
    "        mistake = 1\n",
    "\n",
    "    return alpha, mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_kernel_perceptron(X_test, y_test, alpha, K):\n",
    "    # Calculate the scores for all test samples and classes at once\n",
    "    scores = np.dot(alpha, K.T)\n",
    "\n",
    "    # Predict the class with the highest score for each test sample\n",
    "    y_pred = np.argmax(scores, axis=0)\n",
    "\n",
    "    test_error = np.mean(y_test != y_pred)\n",
    "    return test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 53\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m test_error, train_error, mistakes\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_RUNS):\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Use i_run as random state so each run is reproducable\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m---> 53\u001b[0m         \u001b[43mx_3_data\u001b[49m, y_3_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mi_run\n\u001b[0;32m     54\u001b[0m     )\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# mistakes = np.zeros((len(X_train), len(d)))\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# w = np.zeros(( len(d), len(X_train), len(X_train)))     \u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# K_matrix = polynomial_kernel_3D(X_train, X_train, d)\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# assert K_matrix.shape == (len(d), len(X_train), len(X_train))\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[21], line 53\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m test_error, train_error, mistakes\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_RUNS):\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Use i_run as random state so each run is reproducable\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m---> 53\u001b[0m         \u001b[43mx_3_data\u001b[49m, y_3_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mi_run\n\u001b[0;32m     54\u001b[0m     )\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# mistakes = np.zeros((len(X_train), len(d)))\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# w = np.zeros(( len(d), len(X_train), len(X_train)))     \u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# K_matrix = polynomial_kernel_3D(X_train, X_train, d)\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# assert K_matrix.shape == (len(d), len(X_train), len(X_train))\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\alina\\miniconda3\\envs\\MV00\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alina\\miniconda3\\envs\\MV00\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_RUNS = 1\n",
    "d = np.arange(1, 7, dtype=int)  # k = 1...7\n",
    "#substitute for the real data with 10\n",
    "x_data, y_labels = split_into_data_and_labels(zipcombo)\n",
    "test_error = np.zeros((N_RUNS, len(d)))\n",
    "train_error = np.zeros((N_RUNS, len(d)))\n",
    "#can be extraced from data\n",
    "n_classes = 10\n",
    "nb_epoches_max = 30\n",
    "\n",
    "def run_experiment(x_data, y_labels, d, nb_epoches_max):\n",
    "    for i_run in range(N_RUNS):\n",
    "        # Use i_run as random state so each run is reproducable\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            x_data, y_labels, test_size=0.2, random_state=i_run\n",
    "        )\n",
    "        mistakes = np.zeros((len(X_train), len(d)))\n",
    "        w = np.zeros(( len(d), len(X_train), len(X_train)))     \n",
    "        K_matrix = polynomial_kernel_3D(X_train, X_train, d)\n",
    "        assert K_matrix.shape == (len(d), len(X_train), len(X_train))\n",
    "        for j in range(len(d)):\n",
    "            for e in range(nb_epoches_max):\n",
    "                nb_changes = 0\n",
    "                for i in range(len(X_train)):\n",
    "                    prediction = mysign(np.dot(w[j, :, :], K_matrix[j, i, :]))\n",
    "                    if prediction*Y_train[i] <= 0:\n",
    "                        w[Y_train[i], i, j] += 1\n",
    "                        nb_changes += 1\n",
    "                        mistakes[i, j] += 1\n",
    "                if nb_changes == 0: break;\n",
    "        # Compute the kernel matrix\n",
    "        K_test = polynomial_kernel(X_test, X_train)\n",
    "        K_train = polynomial_kernel(X_train, X_train)\n",
    "        for j in range(len(d)):\n",
    "            # Compute the scores\n",
    "            scores = sum(w[j, i, :] * K_test[i, :] for i in range(len(X_test)))\n",
    "            # Compute the predicted classes\n",
    "            predicted_classes = np.sign(scores)\n",
    "            # Compute the error rate\n",
    "            test_error[i_run, j] = np.mean(predicted_classes != Y_test)\n",
    "            # Compute the scores\n",
    "            scores = sum(w[j, i, :] * K_train[i, :] for i in range(len(X_train)))\n",
    "            # Compute the predicted classes\n",
    "            predicted_classes = np.sign(scores)\n",
    "            # Compute the error rate\n",
    "            train_error[i_run, j] = np.mean(predicted_classes != Y_train)\n",
    "    return test_error, train_error, mistakes\n",
    "\n",
    "\n",
    "for i_run in range(N_RUNS):\n",
    "    # Use i_run as random state so each run is reproducable\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        x_3_data, y_3_labels, test_size=0.2, random_state=i_run\n",
    "    )\n",
    "    # mistakes = np.zeros((len(X_train), len(d)))\n",
    "    # w = np.zeros(( len(d), len(X_train), len(X_train)))     \n",
    "    # K_matrix = polynomial_kernel_3D(X_train, X_train, d)\n",
    "    # assert K_matrix.shape == (len(d), len(X_train), len(X_train))\n",
    "    \n",
    "    print(X_train.shape[0])\n",
    "    # mistakes = np.zeros(( len(d)))\n",
    "    \n",
    "    for j in d:\n",
    "        #all of these can be take out of the loop and add dimension of the polynomial\n",
    "        K = polynomial_kernel(X_train, X_train, j)\n",
    "        w = np.zeros((n_classes, X_train.shape[0]))\n",
    "        print(\"K shape \", K.shape)\n",
    "        for e in range(nb_epoches_max):\n",
    "            # nb_changes = 0\n",
    "            # for i in range(len(X_train)):\n",
    "                #dot product outputs accumulative scores for each class\n",
    "                #argmax returns the index of the class with the highest score\n",
    "                # prediction = np.argmax((np.dot(w, K)))\n",
    "                # if prediction+1 != Y_train[i]:\n",
    "                #     w[Y_train[i]-1, :] += 1\n",
    "                #     w[prediction, :] -= 1\n",
    "                #     nb_changes += 1\n",
    "                #     mistakes[j] += 1\n",
    "            #if nb_changes == 0: break;\n",
    "            for i in range(len(X_train)):\n",
    "                alpha, mistake = train_kernel_perceptron_online(X_train, Y_train, w, K, i)\n",
    "                w = alpha\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0, degree 1, train error 1516.0, number of samples 1859\n",
      "Run 0, degree 1, train error 0.08154922001075847\n",
      "Run 0, degree 1, test error 0.1032258064516129\n",
      "Run 0, degree 2, train error 615.0, number of samples 1859\n",
      "Run 0, degree 2, train error 0.03308230231307154\n",
      "Run 0, degree 2, test error 0.08387096774193549\n",
      "Run 0, degree 3, train error 374.0, number of samples 1859\n",
      "Run 0, degree 3, train error 0.020118343195266272\n",
      "Run 0, degree 3, test error 0.060215053763440864\n",
      "Run 0, degree 4, train error 312.0, number of samples 1859\n",
      "Run 0, degree 4, train error 0.016783216783216783\n",
      "Run 0, degree 4, test error 0.06451612903225806\n",
      "Run 0, degree 5, train error 250.0, number of samples 1859\n",
      "Run 0, degree 5, train error 0.013448090371167294\n",
      "Run 0, degree 5, test error 0.06881720430107527\n",
      "Run 0, degree 6, train error 242.0, number of samples 1859\n",
      "Run 0, degree 6, train error 0.01301775147928994\n",
      "Run 0, degree 6, test error 0.06666666666666667\n",
      "Run 0, degree 7, train error 234.0, number of samples 1859\n",
      "Run 0, degree 7, train error 0.012587412587412588\n",
      "Run 0, degree 7, test error 0.05161290322580645\n",
      "Mean train errors [1.97580224e-27 1.49004292e-23 1.68451411e-19 2.61238412e-15\n",
      " 3.89136384e-11 7.00255593e-07 1.25874126e-02]\n",
      "Mean test errors [0.10322581 0.08387097 0.06021505 0.06451613 0.0688172  0.06666667\n",
      " 0.0516129 ]\n",
      "Mean std train errors [0. 0. 0. 0. 0. 0. 0.]\n",
      "Mean std test errors [0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "N_RUNS = 1\n",
    "d = np.arange(1, 8, dtype=int)  # k = 1...7\n",
    "#substitute for the real data with 10\n",
    "x_data, y_labels = split_into_data_and_labels(zipcombo_qater)\n",
    "#can be extraced from data\n",
    "n_classes = len(np.unique(y_labels))\n",
    "NB_EPOCHS_MAX = 10\n",
    "\n",
    "def run_experiment(x_data, y_labels, N_RUNS, NB_EPOCHS_MAX, d, n_classes):\n",
    "    train_errors = np.zeros((N_RUNS, len(d)))\n",
    "    test_errors = np.zeros((N_RUNS, len(d)))\n",
    "\n",
    "    for i_run in range(N_RUNS):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            x_data, y_labels, test_size=0.2, random_state=i_run\n",
    "        )\n",
    "        n_training_samples = X_train.shape[0]\n",
    "\n",
    "        for j_ind, j in enumerate(d):\n",
    "            K = polynomial_kernel(X_train, X_train, j)\n",
    "            alpha = np.zeros((n_classes, n_training_samples))\n",
    "\n",
    "            for epoch in range(NB_EPOCHS_MAX):\n",
    "                for i in range(len(X_train)):\n",
    "                    alpha, mistake = train_kernel_perceptron_online(X_train, y_train, alpha, K, i, n_classes)\n",
    "                    if mistake:\n",
    "                        train_errors[i_run, j_ind] += 1\n",
    "            print(f\"Run {i_run}, degree {j}, train error {train_errors[i_run, j_ind]}, number of samples {n_training_samples}\")\n",
    "                        \n",
    "            train_errors = train_errors / (NB_EPOCHS_MAX * n_training_samples)\n",
    "            print(f\"Run {i_run}, degree {j}, train error {train_errors[i_run, j_ind]}\")\n",
    "            \n",
    "            # calculating prediction of the test data\n",
    "            K_test = polynomial_kernel(X_test, X_train, j)\n",
    "            test_errors[i_run, j_ind] = predict_kernel_perceptron(X_test, y_test, alpha, K_test)\n",
    "            print(f\"Run {i_run}, degree {j}, test error {test_errors[i_run, j_ind]}\")\n",
    "\n",
    "    mean_train_errors = np.mean(train_errors, axis=0)\n",
    "    print(f\"Mean train errors {mean_train_errors}\")\n",
    "    mean_test_errors = np.mean(test_errors, axis=0)\n",
    "    print(f\"Mean test errors {mean_test_errors}\")\n",
    "    mean_std_train_errors = np.std(train_errors, axis=0)\n",
    "    print(f\"Mean std train errors {mean_std_train_errors}\")\n",
    "    mean_std_test_errors = np.std(test_errors, axis=0)\n",
    "    print(f\"Mean std test errors {mean_std_test_errors}\")\n",
    "    \n",
    "    return mean_train_errors, mean_test_errors, mean_std_train_errors, mean_std_test_errors\n",
    "\n",
    "mean_train_errors, mean_test_errors, mean_std_train_errors, mean_std_test_errors = run_experiment(x_data, y_labels, N_RUNS, NB_EPOCHS_MAX, d, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to compute the polynomial kernel of degree 3\n",
    "def compute_kernel(a, b):\n",
    "    return (np.dot(a, b))**3\n",
    "\n",
    "# Function to compute the sign of a number\n",
    "def compute_sign(x):\n",
    "    return -1.0 if x <= 0.0 else 1.0\n",
    "\n",
    "# Function to initialize the classifier weights\n",
    "def initialize_weights(data):\n",
    "    return np.zeros((3, len(data)))\n",
    "\n",
    "# Function to compute the prediction for a given pattern\n",
    "def compute_prediction(data, pattern, classifier):\n",
    "    return sum(classifier[i] * compute_kernel(pattern, data[i][1:]) for i in range(len(classifier)))\n",
    "\n",
    "# Function to train the classifiers\n",
    "def train_classifiers(data):\n",
    "    global classifier_weights\n",
    "    classifier_weights = initialize_weights(data)\n",
    "    num_mistakes = 0\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        true_class = data[i][0]\n",
    "\n",
    "        # Compute the predictions of the 3 classifiers\n",
    "        predictions = [compute_prediction(data, data[i][1:], classifier_weights[j]) for j in range(3)]\n",
    "        \n",
    "        max_prediction = float(\"-inf\")\n",
    "        for j in range(3):\n",
    "            expected_output = 1.0 if true_class == (j + 1) else -1.0\n",
    "            \n",
    "            # Update the classifier weights if the prediction is incorrect\n",
    "            if expected_output * predictions[j] <= 0:\n",
    "                classifier_weights[j, i] -= compute_sign(predictions[j])\n",
    "            if predictions[j] > max_prediction:\n",
    "                max_prediction = predictions[j]\n",
    "                predicted_class = j + 1\n",
    "                \n",
    "        # Increment the mistake counter if the predicted class is not the true class\n",
    "        if predicted_class != true_class:\n",
    "            num_mistakes += 1\n",
    "\n",
    "    return num_mistakes\n",
    "\n",
    "# Train the model and get the number of mistakes\n",
    "mistakes = train_classifiers(training_data)\n",
    "print(\"Number of mistakes:\", mistakes)\n",
    "\n",
    "# Access the global variable GLBcls after training\n",
    "print(\"GLBcls shape:\", classifier_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table\n",
    "columns = ['method', 'MSE train mean', 'MSE train std', 'MSE test mean', 'MSE test std']\n",
    "\n",
    "df_naive_regression = pd.DataFrame(\n",
    "    [[\n",
    "        'Naive Regression', train_error.mean(), train_error.std(), test_error.mean(), test_error.std()\n",
    "    ]],\n",
    "    columns=columns\n",
    ")\n",
    "df_naive_regression.set_index('method', inplace=True)\n",
    "df_naive_regression.to_latex('naive_regression.tex', float_format=\"{:.2f}\".format)\n",
    "\n",
    "# Take an average over all the runs\n",
    "df_single_attr_regression = pd.concat(\n",
    "    (\n",
    "        pd.DataFrame(train_error.mean(axis=0), columns=[columns[1]]),\n",
    "        pd.DataFrame(train_error.std(axis=0), columns=[columns[2]]),\n",
    "        pd.DataFrame(test_error.mean(axis=0), columns=[columns[3]]),\n",
    "        pd.DataFrame(test_error.std(axis=0), columns=[columns[4]])),\n",
    "    axis=1\n",
    ")\n",
    "# Create the labels for each attribute\n",
    "df_single_attr_regression['method'] = [f'Linear Regression (attribute {i})' for i in df_single_attr_regression.index]\n",
    "df_single_attr_regression.set_index('method', inplace=True)\n",
    "df_single_attr_regression.to_latex('single_attr_regression.tex', float_format=\"{:.2f}\".format)\n",
    "\n",
    "df_all_attrs_regression = pd.DataFrame(\n",
    "    [[\n",
    "        'Linear Regression (all attributes)', mse_all_train.mean(), mse_all_train.std(), mse_all_test.mean(),\n",
    "        mse_all_test.std()\n",
    "    ]],\n",
    "    columns=columns\n",
    ")\n",
    "df_all_attrs_regression.set_index('method', inplace=True)\n",
    "df_all_attrs_regression.to_latex('all_attrs_regression.tex', float_format=\"{:.2f}\".format)\n",
    "\n",
    "# Combine into a summary table for all the linear regression methods\n",
    "df_linear_summary = pd.concat((df_naive_regression, df_single_attr_regression, df_all_attrs_regression), axis=0)\n",
    "df_linear_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MV00",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
